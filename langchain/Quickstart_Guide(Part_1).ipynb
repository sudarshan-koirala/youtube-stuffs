{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDFeU+/qTmOlejGoG52XTA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/langchain/Quickstart_Guide(Part_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quickstart Guide (part 1)\n",
        "### [Youtube Video Covering this Notebook](https://youtu.be/gVMp_vKwslI)\n",
        "# Building a Language Model Application: LLMs\n",
        "- https://python.langchain.com/en/latest/getting_started/getting_started.html\n",
        "- This quickstart guide provides you a quick walkthrough about building an end-to-end language model application with LangChain\n",
        "- This helps you to know high level understanding of what the framework is capable of."
      ],
      "metadata": {
        "id": "aIVTlfviLK03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "H2wSBdVwMEHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai huggingface_hub watermark --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQyILM-6Lufj",
        "outputId": "46c66f30-8841-498f-e35a-3bec260aaf72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m626.5/626.5 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark\n",
        "%watermark -a \"Sudarshan Koirala\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSCVHcEYS7FA",
        "outputId": "c1262455-549d-4c74-c0f7-7937168f9677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sudarshan Koirala\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "- Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n",
        "- <font color=\"red\">HuggingfaceHub API (its free)</font>\n",
        "- <font color=\"red\">OpenAI's API (its not free)</font>\n",
        "\n",
        "**Get api keys (you need to create account for both to access the api keys)**\n",
        "\n",
        "- Get OpenAI api key: https://platform.openai.com/account/api-keys\n",
        "- Huggingface api key: https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "xWSS4cN7M3Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"HUGGINGFACEHUB_API_TOKEN\"\n"
      ],
      "metadata": {
        "id": "W19xmh93Pr45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“NOTE: LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications."
      ],
      "metadata": {
        "id": "2ZVZuC0VOVm6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_hXfobDCPA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs: Get Prediction from a language model\n",
        "- This is the most basic building block of LangChain.\n",
        "- Provides a uniform interface to interact with different large langauge models."
      ],
      "metadata": {
        "id": "PmsfPQxgO32D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "2qoYayO4Mk3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI??"
      ],
      "metadata": {
        "id": "dHiefMA7Q_v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The temperature argument in the OpenAI LLM wrapper is used to <font color=\"red\">control the level of randomness in the generated text.</font>\n",
        "- <font color=\"cyan\">A higher temperature value will result in more diverse and unpredictable text, while a lower temperature value will result in more conservative and predictable text. </font> \n",
        "- The default value for temperature is 1.0, and valid values range from 0.0 to 1.0."
      ],
      "metadata": {
        "id": "p5JsKY0EUzFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the wrapper around OpenaI LLMs and call it on some input\n",
        "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\"\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1hIkwQ3QG-_",
        "outputId": "5e012f00-c3b4-49a6-af4b-e6fc64576f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Rainbow Socks Co.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The default Hugging Face Hub inference APIs do not use specialized hardware. They are also not suitable for running larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)\n",
        "- You can try changing the runtime in google colab to see how it performs."
      ],
      "metadata": {
        "id": "coLUy0KKZb7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HuggingFaceHub??"
      ],
      "metadata": {
        "id": "BGkpjmP0WLch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the wrapper around HuggingfaceHub models and call it on some input\n",
        "hub_llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.9, \"max_length\":64}) # model_name=\"text-davinci-003\"\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(hub_llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52qx9JY6Tqc6",
        "outputId": "719f9432-ef80-44ea-ea66-d66ba8948eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sock monkey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6kIwuIoCQo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates: Mange prompts for LLMs\n",
        "- <font color=\"orange\">Instead of hard-coding thet text we want to aks, we can use prompt template to manage the prompt.</font> \n",
        "- Also, when creating application, its not feasible to pass the input directly to LLM.\n",
        "- It's similar to `python f-strings`."
      ],
      "metadata": {
        "id": "_aYG3masavjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# python f-string example\n",
        "framework = \"LangChain\"\n",
        "print(f\"I'm learning {framework}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV4ZjNhNosGH",
        "outputId": "83f28afc-5703-4402-873d-48678f4a19d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm learning LangChain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "hZSJiNadW1EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now just calling the `.format` method will format it.\n",
        "print(prompt.format(product=\"colorful socks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yux9wGENbXM5",
        "outputId": "cf414736-b3f6-43e5-c1e9-eec1dec56117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is a good name for a company that makes colorful socks?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AB0MsxMCRYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains: Combine LLMs and Prompts in multi-step workflows\n",
        "- In real application, Using LLM in isolation is OK for some applications but in most of the cases it requires chaining. And chaining with PromptTemplate might be a neccessity. \n",
        "- A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains\n",
        "- <font color=\"red\">Extending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.</font>"
      ],
      "metadata": {
        "id": "4u8hfDLuqO80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PromptTemplate??"
      ],
      "metadata": {
        "id": "ruwNQohttvJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hub_llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.9, \"max_length\":64}) # model_name=\"text-davinci-003\"\n",
        "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "atpILVEPbpDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a simple chain that takes user input, format the prompt with it and send it to LLM\n",
        "from langchain.chains import LLMChain\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "WbdogSW2uEZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now run the chain only specifying the product\n",
        "#chain.run(\"colorful socks\")\n",
        "print(llm_chain.run(\"colorful socks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE2Mhr_2ukqO",
        "outputId": "e4f97116-80a7-411e-ef59-8d93afd4728a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sock Starz.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-k_l0l8wCSNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents: Dynamically call chains based on user input\n",
        "- So, far what we did was to run the chains in a predetermined order.\n",
        "- <font color =\"orange\"> Agents can use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning to the user.</font>\n",
        "\n",
        "In order to load agents, understanding the following concepts is crucial.\n",
        "\n",
        "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "\n",
        "- LLM: The language model powering the agent.\n",
        "\n",
        "- Agents: Agents involve an LLM making decisions about which actions to take, taking that Action, seeing an observation, and repeating that until its done.\n",
        "\n",
        "Agents: For a list of supported agents and their specifications, see [here](https://python.langchain.com/en/latest/modules/agents/agents.html).\n",
        "\n",
        "Tools: For a list of predefined tools and their specifications, see [here](https://python.langchain.com/en/latest/modules/agents/tools.html).\n",
        "\n",
        "<font color=\"red\">For this example, you will also need to install the [SerpAPI Python package](https://pypi.org/project/google-search-results/).</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "IQBWXNR4z-cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axoJAs402uQS",
        "outputId": "362d78fb-4c1a-4ee5-a65d-6da89624b87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://serpapi.com/\n",
        "import os\n",
        "os.environ['SERPAPI_API_KEY'] = \"SERPAPI_API_KEY\""
      ],
      "metadata": {
        "id": "ihxWArPc4L_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "#tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "tools = load_tools([\"serpapi\"])\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"What is LangChain ?\")\n",
        "#agent.run(\"What was the high temperature in Helsinki yesterday in Celsius? What is that number in Fahrenheit ?\")\n",
        "#agent.run(\"Who is the president of Finland ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "FdiZ7Ueg6fBv",
        "outputId": "9ee7754e-79ee-44ec-84fb-d45adb0e536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should research this to find out more information.\n",
            "Action: Search\n",
            "Action Input: \"LangChain\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This looks like a good source of information.\n",
            "Action: Search\n",
            "Action Input: \"LangChain applications\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This looks like enough information to answer the question.\n",
            "Final Answer: LangChain is a framework for developing applications powered by language models. It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a framework for developing applications powered by language models. It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Bmy4qnPCTYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory: Add State to Chains and Agents\n",
        "- So far, all the chains and agents weâ€™ve gone through have been stateless. But often, you may want a chain or agent to have some concept of â€œmemoryâ€ so that it may remember information about its previous interactions.\n",
        "- For example, while designing a chatbot you want it to remember previous message or previous several messages.\n",
        "- Short-term memory\n",
        "- Long-term-memory (remembering key pieces of information over time)\n"
      ],
      "metadata": {
        "id": "Sqj9WHbK-VGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "output = conversation.predict(input=\"Hi there!\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAnoBgSv7SVC",
        "outputId": "9013ab4c-1da6-4206-db7a-a90d1ad07530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hi there! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAT19mCV-8u2",
        "outputId": "973a2b1c-a907-4d11-cfbf-bcbabb41e8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3iNCgxm_A4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZw2K7M/AjT+BeF+DKgYK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93bdfcd316b94785a9f63b54fb2e7eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_194fcb196a394e8cb78d9e39ef9461c0",
            "placeholder": "Please enter your question:",
            "style": "IPY_MODEL_87f711d699b443eeac5301e31e139210",
            "value": ""
          }
        },
        "194fcb196a394e8cb78d9e39ef9461c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f711d699b443eeac5301e31e139210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5434caeb5dc840a0a1996b7f1151e9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52fddac56425425fb1b3f6c41a83ad40",
            "placeholder": "​",
            "style": "IPY_MODEL_6119c20941e645fe85777d8a40134f45",
            "value": "<b>User:</b> who are the authors of gpt4al"
          }
        },
        "52fddac56425425fb1b3f6c41a83ad40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6119c20941e645fe85777d8a40134f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ebbefa2c67d44a6bb7a1ceaf61f9b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da35600790c34a9db211e75bbd47d00b",
            "placeholder": "​",
            "style": "IPY_MODEL_0a59ae49aefe454dac82fd3cfa509201",
            "value": "<b><font color=\"Orange\">Chatbot:</font></b>  The authors of GPT4All are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin M. Schmidt, Adam Treat, and Andriy Mulyar."
          }
        },
        "da35600790c34a9db211e75bbd47d00b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a59ae49aefe454dac82fd3cfa509201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c92fcac4b2b44dabe4210335e479af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d30bddef5eff4ba39adbfc977d1b543d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f22048167ab4e6a99ff352e4881d6f5",
            "value": "<b>User:</b> what is pandas ai "
          }
        },
        "d30bddef5eff4ba39adbfc977d1b543d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f22048167ab4e6a99ff352e4881d6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8217b151f57a44d68f2135fba059de9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96f4fd5d75874fa3a9ba1f6d74b86855",
            "placeholder": "​",
            "style": "IPY_MODEL_cb41fc15e4e84ddea00a1a63544486b6",
            "value": "<b><font color=\"Orange\">Chatbot:</font></b> \n\nPandas AI is a Python library that adds generative artificial intelligence capabilities to Pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with Pandas, and is not a replacement for it."
          }
        },
        "96f4fd5d75874fa3a9ba1f6d74b86855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb41fc15e4e84ddea00a1a63544486b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/langchain/Chat_with_Any_Documents_Own_ChatGPT_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with any documents using langchain\n",
        "\n",
        "#### [Youtube video covering this notebook](https://youtu.be/TeDgIDqQmzs)"
      ],
      "metadata": {
        "id": "vmYA8nmCMfOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[OpenAI token limit](https://platform.openai.com/docs/models/gpt-4)  \n",
        "OpenAI's embedding model has 1536 dimensions.  \n",
        "After the data is turned into embeddings, they are stored in a vectorstore database, such as Pinecone, Chroma and Faiss, etc.  \n",
        "Once the query is provided, the most relevant chunks of data is queried based on the similarity (semantic search)  \n"
      ],
      "metadata": {
        "id": "6_LAfYA_N1E2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "43Z6U7XNMl0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S6k29lnn-hF7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai langchain  tiktoken pypdf unstructured[local-inference] gradio chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext watermark\n",
        "%watermark -a \"Sudarshan Koirala\" -vmp langchain,openai,chromadb"
      ],
      "metadata": {
        "id": "TKuV8k5wDhpf",
        "outputId": "f30267e4-bc3c-42a4-c2e9-b7f69f396d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sudarshan Koirala\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.10.11\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "langchain: 0.0.162\n",
            "openai   : 0.27.6\n",
            "chromadb : 0.3.22\n",
            "\n",
            "Compiler    : GCC 9.4.0\n",
            "OS          : Linux\n",
            "Release     : 5.10.147+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Pinecone, Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "EL9SFDqQ_F-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] =\"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "v_3FRN1dAbGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "8cQ1IYdlAkb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LangChain Document Loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)"
      ],
      "metadata": {
        "id": "CVKH8f-mqowj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "pdf_loader = DirectoryLoader('/content/Documents/', glob=\"**/*.pdf\")\n",
        "readme_loader = DirectoryLoader('/content/Documents/', glob=\"**/*.md\")\n",
        "txt_loader = DirectoryLoader('/content/Documents/', glob=\"**/*.txt\")"
      ],
      "metadata": {
        "id": "9bCdJGpTB8Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#take all the loader\n",
        "loaders = [pdf_loader, readme_loader, txt_loader]\n",
        "\n",
        "#lets create document \n",
        "documents = []\n",
        "for loader in loaders:\n",
        "    documents.extend(loader.load())"
      ],
      "metadata": {
        "id": "JYkYmqmEDPwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77957cb-eb05-4848-a575-824c2872d1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:unstructured:detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'You have {len(documents)} document(s) in your data')\n",
        "print (f'There are {len(documents[0].page_content)} characters in your document')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khg3Wm8zrNvm",
        "outputId": "ad4df0a6-db16-4d46-d2ab-98e633af443a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 3 document(s) in your data\n",
            "There are 10701 characters in your document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "wx0j1rnwGW48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a9f956-dc7d-4c64-c4b7-d19ac607b648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='GPT4All\\n\\n\\n\\nJ: An Apache\\n\\n\\n\\n2 Licensed Assistant\\n\\n\\n\\nStyle Chatbot\\n\\nYuvanesh Anand\\n\\nyuvanesh@nomic.ai\\n\\nZach Nussbaum\\n\\nzach@nomic.ai\\n\\nBrandon Duderstadt\\n\\nbrandon@nomic.ai\\n\\nBenjamin M. Schmidt\\n\\nben@nomic.ai\\n\\nAdam Treat\\n\\ntreat.adam@gmail.com\\n\\nAndriy Mulyar\\n\\nandriy@nomic.ai\\n\\nAbstract\\n\\nGPT4All-J is an Apache-2 licensed chatbot trained over a massive curated corpus of as- sistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories. It builds on the March 2023 GPT4All release by training on a significantly larger corpus, by deriving its weights from the Apache-licensed GPT-J model rather than the GPL-licensed of LLaMA, and by demonstrat- ing improved performance on creative tasks such as writing stories, poems, songs and plays. We openly release the training data, data curation procedure, training code, and fi- nal model weights to promote open research and reproducibility. Additionally, we release Python bindings and a Chat UI to a quantized 4-bit version of GPT4All-J allowing virtually anyone to run the model on CPU.\\n\\n1 Data Collection and Curation\\n\\nWe gather a diverse sample of questions/prompts by leveraging several publicly available datasets and curating our own set of prompts:\\n\\nSeveral\\n\\nsubsamples\\n\\nfrom subsets\\n\\nLAION OIG including\\n\\nunified unifiedskg instruction,\\n\\nfied hc3 human,\\n\\nunified abstract infill\\n\\nunified multi news\\n\\nof\\n\\nunified chip2,\\n\\nuni\\n\\n\\n\\nand\\n\\nCoding questions with a random sub-sample\\n\\nof Stackoverflow Questions\\n\\nInstruction-tuning with a sub-sample of Big-\\n\\nscience/P3\\n\\nCustom-generated creative questions.\\n\\nWe accompany this paper with the 800k point GPT4All-J dataset that is a superset of the origi- nal 400k points GPT4All dataset. We dedicated substantial attention to data preparation and cura- tion.\\n\\nBuilding on the GPT4All dataset, we curated the GPT4All-J dataset by augmenting the origi- nal 400k GPT4All examples with new samples encompassing additional multi-turn QA samples and creative writing such as poetry, rap, and short stories. We designed prompt templates to create different scenarios for creative writing. The cre- ative prompt template was inspired by Mad Libs style variations of ‘Write a [creative story type] about [NOUN] in the style of [PERSON]‘. In ear- lier versions of GPT4All, we found that rather than writing actual creative content, the model would discuss how it would go about writing the content. Training on this new dataset allows GPT4All-J to write poems, songs, and plays with increased com- petence.\\n\\nWe used Atlas to inform our data cleaning and curation efforts. We started with a collection of ap- proximately 1,000,000 points. Several data cura- tion iterations produced our final GPT4All-J train- ing set. Among other changes, we removed exact duplicate prompts and responses characterized by homogeneous clusters in the Atlas map. We also removed prompts that were less than 10 characters such as single words like ‘The‘, ‘And‘, as well as poorly formatted examples.\\n\\nInteractively explore the cleaned dataset in At-\\n\\nlas:\\n\\nGPT4All-J Curated Training Set Map\\n\\n2 Model Training\\n\\nWe trained several models finetuned from both LLaMA 7B (Touvron et al., 2023) and GPT-J (Wang and Komatsuzaki, 2021) checkpoints. The model associated with our initial public release is trained with LoRA (Hu et al., 2021) on the 437,605 post-processed examples for four epochs while the finetuned GPT-J was trained for one epoch. Detailed model hyper-parameters and training code can be found in the associated repos-\\n\\nModel\\n\\nGPT4All\\n\\n\\n\\nJ 6.7B\\n\\nGPT4All\\n\\n\\n\\nJ Lora 6.7B\\n\\nGPT4All LLaMa Lora 7B\\n\\nDolly 6B\\n\\nDolly 12B\\n\\nAlpaca 7B\\n\\nAlpaca Lora 7B\\n\\nGPT\\n\\n\\n\\nJ 6.7B\\n\\nLLaMa 7B\\n\\nPythia 6.7B\\n\\nPythia 12B\\n\\nBoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c OBQA 64.7 73.4 63.5 68.6 67.8 73.1 63.9 68.8 62.2 56.7 66.1 73.9 68.8 74.3 64.1 65.4 66.9 73.1 61.1 63.5 63.8 67.7\\n\\n40.2\\n\\n40.2\\n\\n40.2\\n\\n41.2\\n\\n40.4\\n\\n43.4\\n\\n42.6\\n\\n38.2\\n\\n42.4\\n\\n37.2\\n\\n38\\n\\n74.8\\n\\n75.8\\n\\n77.6\\n\\n77.3\\n\\n75.4\\n\\n77.2\\n\\n79.3\\n\\n76.2\\n\\n77.4\\n\\n76.3\\n\\n76.6\\n\\n63.4\\n\\n66.2\\n\\n72.1\\n\\n67.6\\n\\n71.0\\n\\n73.9\\n\\n74.0\\n\\n66.2\\n\\n73.0\\n\\n64.0\\n\\n67.3\\n\\n54.9\\n\\n56.4\\n\\n51.1\\n\\n62.9\\n\\n64.6\\n\\n59.8\\n\\n56.6\\n\\n62.2\\n\\n52.5\\n\\n61.3\\n\\n63.9\\n\\n36.0\\n\\n35.7\\n\\n40.4\\n\\n38.7\\n\\n38.5\\n\\n43.3\\n\\n43.9\\n\\n36.6\\n\\n41.4\\n\\n35.2\\n\\n34.8\\n\\nTable 1: Zero-shot performance on Common Sense Reasoning tasks\\n\\n(a) TSNE visualization of the final GPT4All-J training data, ten-colored by extracted topic.\\n\\n(b) Zoomed in view of Figure 1a. The region displayed con- tains generations related to personal health and wellness.\\n\\nFigure 1: The final training data was curated to ensure a diverse distribution of prompt topics and model responses. View online\\n\\nitory and model training log. We additionally re- lease both GPT-J and GPT-J LoRa checkpoints. Updates to the training log were made to include the additional experiments run for GPT-J.\\n\\n2.1 Reproducibility\\n\\nWe release all data, training code and logs for the community to learn, build and benefit from. Please check the Git repository for the most up-to-date data, training details and checkpoints.\\n\\nthe training samples that we openly release to the community. Our released model, GPT4All-J, can be trained in about eight hours on a Paperspace DGX A100 8x 80GB for a total cost of $200. Us- ing a government calculator, we estimate the final model training to produce the equivalent of 0.18 metric tons of carbon dioxide, roughly equivalent to that produced by burning 20 gallons (75 liters) of gasoline.\\n\\n3 Evaluation\\n\\n2.2 Costs\\n\\nRunning all of our experiments cost about $5000 in GPU costs. We gratefully acknowledge our compute sponsor Paperspace for their generosity in making GPT4All-J training possible. Between GPT4All and GPT4All-J, we have spent about $800 in OpenAI API credits so far to generate\\n\\nWe perform a preliminary evaluation of our model using the human evaluation data from the Self- Instruct paper (Wang et al., 2022). We report the ground truth perplexity of our model against what is, to our knowledge, the best openly available alpaca-lora model, provided by user chainyo on huggingface. We find that all models have very\\n\\nlarge perplexities on a small number of tasks, and report perplexities clipped to a maximum of 100. Models fine-tuned on this collected dataset ex- hibit much lower perplexity in the Self-Instruct evaluation compared to Alpaca. This evaluation is in no way exhaustive and further evaluation work remains. We welcome the reader to run the model locally on CPU (see Github for files).\\n\\n3.1 Common Sense Reasoning\\n\\nFollowing results from (Conover et al.), we evalu- ate on 7 standard common sense reasoning tasks: ARC easy and challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), OpenBookQA (Mihaylov et al., 2018), and Winogrande (Sakaguchi et al., 2019). We evaluate several models: GPT-J (Wang and Komatsuzaki, 2021), Pythia (6B and 12B) (Bi- derman et al., 2023), Dolly v1 and v2 (Conover et al.), and GPT4All using lm-eval-harness (Gao et al., 2021). Similar to results in (Ouyang et al., 2022), instruction-tuning showed performance re- gressions over the base model. However, we no- tice in some tasks that the LoRA instruction fine- tuned models show some performance improve- ments.\\n\\n4 Use Considerations\\n\\nThe authors release data and training details in hopes that it will accelerate open LLM research, particularly in the domains of fairness, align- ment, interpretability, and transparency. GPT4All- J model weights and quantized versions are re- leased under an Apache 2 license and are freely available for use and distribution. Please note that the less restrictive license does not apply to the original GPT4All model that is based on LLaMA, which has a non-commercial GPL license. The assistant data was gathered from OpenAI’s GPT- 3.5-Turbo, whose terms of use prohibit developing models that compete commercially with OpenAI.\\n\\nReferences\\n\\nStella Biderman, Hailey Schoelkopf, Quentin An- thony, Herbie Bradley, Kyle O’Brien, Eric Hal- lahan, Mohammad Aflah Khan, Shivanshu Puro- hit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling.\\n\\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\\n\\nTom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question an- swering? try arc, the ai2 reasoning challenge.\\n\\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali Gh- odsi, Patrick Wendell, Matei Zaharia, and et al. Free dolly: Introducing the world’s first truly open instruction-tuned llm.\\n\\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Gold- ing, Jeffrey Hsu, Kyle McDonell, Niklas Muen- nighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation.\\n\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models.\\n\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct elec- tricity? a new dataset for open book question an- swering. In EMNLP.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.\\n\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga- vatula, and Yejin Choi. 2019. Winogrande: An ad- versarial winograd schema challenge at scale.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\\n\\nBen Wang and Aran Komatsuzaki. 2021.\\n\\nGPT- J-6B: A 6 Billion Parameter Autoregressive https://github.com/ Language Model. kingoflolz/mesh-transformer-jax.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al- isa Liu, Noah A. Smith, Daniel Khashabi, and Han- naneh Hajishirzi. 2022. Self-instruct: Aligning lan- guage model with self generated instructions.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence?', metadata={'source': '/content/Documents/gpt4all.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Text from the documents"
      ],
      "metadata": {
        "id": "01sK31QQGuQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=40) #chunk overlap seems to work better\n",
        "documents = text_splitter.split_documents(documents)\n",
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS6iEnTxGkku",
        "outputId": "ba436850-2429-4379-d92a-21ceb34b4d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-pByaC1HHR9",
        "outputId": "d3bd31bc-43be-49c9-d521-bf28ccae2ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='GPT4All\\n\\n\\n\\nJ: An Apache\\n\\n\\n\\n2 Licensed Assistant\\n\\n\\n\\nStyle Chatbot\\n\\nYuvanesh Anand\\n\\nyuvanesh@nomic.ai\\n\\nZach Nussbaum\\n\\nzach@nomic.ai\\n\\nBrandon Duderstadt\\n\\nbrandon@nomic.ai\\n\\nBenjamin M. Schmidt\\n\\nben@nomic.ai\\n\\nAdam Treat\\n\\ntreat.adam@gmail.com\\n\\nAndriy Mulyar\\n\\nandriy@nomic.ai\\n\\nAbstract', metadata={'source': '/content/Documents/gpt4all.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwYWEYf-HLF6",
        "outputId": "3081f1ce-4875-4792-a72c-829a7dd43236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='GPT4All-J is an Apache-2 licensed chatbot trained over a massive curated corpus of as- sistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories. It builds on the March 2023 GPT4All release by training on a significantly larger corpus, by deriving its weights from the Apache-licensed GPT-J model rather than the GPL-licensed of LLaMA, and by demonstrat- ing improved performance on creative tasks such as writing stories, poems, songs and plays. We openly release the training data, data curation procedure, training code, and fi- nal model weights to promote open research and reproducibility. Additionally, we release Python bindings and a Chat UI to a quantized 4-bit version of GPT4All-J allowing virtually anyone to run the model on CPU.\\n\\n1 Data Collection and Curation\\n\\nWe gather a diverse sample of questions/prompts by leveraging several publicly available datasets and curating our own set of prompts:\\n\\nSeveral\\n\\nsubsamples\\n\\nfrom subsets', metadata={'source': 'Documents/gpt4all.pdf'})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings and storing it in Vectorestore"
      ],
      "metadata": {
        "id": "Wx6JtYgWHZvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "XI--vvaJHOgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Chroma for storing vectors"
      ],
      "metadata": {
        "id": "qHqY2lzsL-Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "r6AUBDQ_hunC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zOKdK4shnzc",
        "outputId": "2ed81422-bc9a-4816-a337-556d675fa204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using pinecone for storing vectors"
      ],
      "metadata": {
        "id": "bdILt9zkMGWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pinecone-client"
      ],
      "metadata": {
        "id": "0uzBpI2sHpCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Pinecone langchain doc](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html?highlight=pinecone#pinecone\n",
        ")\n",
        "- What is [vectorstore](https://www.pinecone.io/learn/vector-database/)\n",
        "- Get your pinecone api key and env -> https://app.pinecone.io/"
      ],
      "metadata": {
        "id": "3gbHwwRgI5n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "PINECONE_API_KEY = getpass.getpass('Pinecone API Key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Da55fnPIDwE",
        "outputId": "661ce054-9981-4367-b4e4-47fccada936c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_ENV = getpass.getpass('Pinecone Environment:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ6sLRkMJZCm",
        "outputId": "d7954545-e4c8-4f9f-e22b-9473e150c71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone Environment:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone \n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_ENV  # next to api key in console\n",
        ")\n",
        "\n",
        "index_name = \"langchain-demo\"\n",
        "\n",
        "vectorstore = Pinecone.from_documents(documents, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "OYsPAF3TJhmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you already have an index, you can load it like this\n",
        "import pinecone\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_ENV  # next to api key in console\n",
        ")\n",
        "\n",
        "index_name = \"langchain-demo\"\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embeddings)"
      ],
      "metadata": {
        "id": "a4lCSdF6fTRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We had 23 documents so there are 23 vectors being created in Pinecone."
      ],
      "metadata": {
        "id": "N-7i5EkcNBnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who are the authors of gpt4all paper ?\"\n",
        "docs = vectorstore.similarity_search(query)"
      ],
      "metadata": {
        "id": "dbUFZWoXJ3d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs) #it went on and search on the 4 different vectors to find the similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSqHUoQRNmCI",
        "outputId": "3fedd512-9c1e-4f43-d493-beda1d8a49e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmSZQsDENfiE",
        "outputId": "37363217-3098-4f2c-c646-8d991502abf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 Use Considerations\n",
            "\n",
            "The authors release data and training details in hopes that it will accelerate open LLM research, particularly in the domains of fairness, align- ment, interpretability, and transparency. GPT4All- J model weights and quantized versions are re- leased under an Apache 2 license and are freely available for use and distribution. Please note that the less restrictive license does not apply to the original GPT4All model that is based on LLaMA, which has a non-commercial GPL license. The assistant data was gathered from OpenAI’s GPT- 3.5-Turbo, whose terms of use prohibit developing models that compete commercially with OpenAI.\n",
            "\n",
            "References\n",
            "\n",
            "Stella Biderman, Hailey Schoelkopf, Quentin An- thony, Herbie Bradley, Kyle O’Brien, Eric Hal- lahan, Mohammad Aflah Khan, Shivanshu Puro- hit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyoXLl0YUqz1",
        "outputId": "d2af77a9-e59a-4d74-fe4a-da51e95ec8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building on the GPT4All dataset, we curated the GPT4All-J dataset by augmenting the origi- nal 400k GPT4All examples with new samples encompassing additional multi-turn QA samples and creative writing such as poetry, rap, and short stories. We designed prompt templates to create different scenarios for creative writing. The cre- ative prompt template was inspired by Mad Libs style variations of ‘Write a [creative story type] about [NOUN] in the style of [PERSON]‘. In ear- lier versions of GPT4All, we found that rather than writing actual creative content, the model would discuss how it would go about writing the content. Training on this new dataset allows GPT4All-J to write poems, songs, and plays with increased com- petence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "etsPkZROVpo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now the langchain part (Chaining with Chat History) --> With One line of Code (Fantastic)\n",
        "- There are many chains but we use this [link](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html)"
      ],
      "metadata": {
        "id": "keEibikCN6-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "IiSQ4IQCtDT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), retriever)"
      ],
      "metadata": {
        "id": "_ZGMwOuaNiHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1TnRF83PMZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "query = \"How much is spent for training the gpt4all model?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LxpEcvZYoN3D",
        "outputId": "9b0c3539-9e1d-49f6-8e69-c8798680e391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' $200'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append((query, result[\"answer\"]))\n",
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3QZzJ0D-6QY",
        "outputId": "67519e6c-488c-4948-d992-10236fab0a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('How much is spent for training the gpt4all model?', ' $200')]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is this number multiplied by 2?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wtn22jDK_6Qj",
        "outputId": "eb65e3e9-b1a4-4680-c8b6-e7b215b4cc7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' $1600'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a chatbot with memory with simple widgets"
      ],
      "metadata": {
        "id": "X5oduK4NVv1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "vYMWTZoxV1Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "    \n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Thanks for the chat!\")\n",
        "        return\n",
        "    \n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "    \n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"Orange\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"Chat with your data. Type 'exit' to stop\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='Please enter your question:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "93bdfcd316b94785a9f63b54fb2e7eed",
            "194fcb196a394e8cb78d9e39ef9461c0",
            "87f711d699b443eeac5301e31e139210",
            "5434caeb5dc840a0a1996b7f1151e9d1",
            "52fddac56425425fb1b3f6c41a83ad40",
            "6119c20941e645fe85777d8a40134f45",
            "7ebbefa2c67d44a6bb7a1ceaf61f9b63",
            "da35600790c34a9db211e75bbd47d00b",
            "0a59ae49aefe454dac82fd3cfa509201",
            "9c92fcac4b2b44dabe4210335e479af6",
            "d30bddef5eff4ba39adbfc977d1b543d",
            "3f22048167ab4e6a99ff352e4881d6f5",
            "8217b151f57a44d68f2135fba059de9f",
            "96f4fd5d75874fa3a9ba1f6d74b86855",
            "cb41fc15e4e84ddea00a1a63544486b6"
          ]
        },
        "id": "Q88JFgfLV9hQ",
        "outputId": "b262ee30-8863-46b5-e6e0-7160c6f17bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat with your data. Type 'exit' to stop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', placeholder='Please enter your question:')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93bdfcd316b94785a9f63b54fb2e7eed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b>User:</b> who are the authors of gpt4al')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5434caeb5dc840a0a1996b7f1151e9d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b>  The authors of GPT4All are Yuvanesh Anand, Zach Nussb…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ebbefa2c67d44a6bb7a1ceaf61f9b63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b>User:</b> what is pandas ai ')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c92fcac4b2b44dabe4210335e479af6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<b><font color=\"Orange\">Chatbot:</font></b> \\n\\nPandas AI is a Python library that adds generative…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8217b151f57a44d68f2135fba059de9f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Part (Building the [chatbot like UI](https://gradio.app/docs/#chatbot))"
      ],
      "metadata": {
        "id": "_rVF__4XPdwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio sample example"
      ],
      "metadata": {
        "id": "IJ0kvK0tOt_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        print(message)\n",
        "        print(chat_history)\n",
        "        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
        "        chat_history.append((message, bot_message))\n",
        "        print(chat_history)\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "VoKW01NoOsml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "e6719dab-8e98-4107-9522-d5cb482caddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1dbfbc6e387c4c0006.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1dbfbc6e387c4c0006.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "[]\n",
            "[('hello', 'I love you')]\n",
            "hi\n",
            "[['hello', 'I love you']]\n",
            "[['hello', 'I love you'], ('hi', 'How are you?')]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://1dbfbc6e387c4c0006.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio langchain example"
      ],
      "metadata": {
        "id": "1wDesKT8Oz8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    \n",
        "    def respond(user_message, chat_history):\n",
        "        print(user_message)\n",
        "        print(chat_history)\n",
        "        # Get response from QA chain\n",
        "        response = qa({\"question\": user_message, \"chat_history\": chat_history})\n",
        "        # Append user message and response to chat history\n",
        "        chat_history.append((user_message, response[\"answer\"]))\n",
        "        print(chat_history)\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "Ug2RL9rdPXa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "095d2936-a255-4e17-8262-60014471c474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://42d679ac88ec3d1362.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://42d679ac88ec3d1362.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "[]\n",
            "[('hello', \" I'm sorry, I don't know the answer to that question.\")]\n",
            "who are the authors of gpt4all paper.\n",
            "[['hello', ' I’m sorry, I don’t know the answer to that question.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 399, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1299, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1022, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"<ipython-input-68-74e405dd0daf>\", line 11, in respond\n",
            "    response = qa({\"question\": user_message, \"chat_history\": chat_history})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 142, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 136, in __call__\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\", line 97, in _call\n",
            "    chat_history_str = get_chat_history(inputs[\"chat_history\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\", line 45, in _get_chat_history\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported chat history format: <class 'list'>. Full chat history: [['hello', ' I’m sorry, I don’t know the answer to that question.']] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://42d679ac88ec3d1362.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9V1nvnA__OA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}